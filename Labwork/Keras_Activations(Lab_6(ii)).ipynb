{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-6(ii)-Keras-Activations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devesh-Patodia/DL-J038/blob/labwork/Lab_6(ii)_Keras_Activations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmQKx4diH0vV",
        "colab_type": "text"
      },
      "source": [
        "#Usage of activations\n",
        "\n",
        "Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from keras.layers import Activation, Dense\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('tanh'))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "This is equivalent to:\n",
        "\n",
        "```\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "```\n",
        "\n",
        "You can also pass an element-wise TensorFlow/Theano/CNTK function as an activation:\n",
        "\n",
        "```\n",
        "from keras import backend as K\n",
        "model.add(Dense(64, activation=K.tanh))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcMWdTDBIik-",
        "colab_type": "text"
      },
      "source": [
        "#elu\n",
        "\n",
        "keras.activations.elu(x, alpha=1.0)\n",
        "\n",
        "Exponential linear unit.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "2. alpha: A scalar, slope of negative section.\n",
        "\n",
        "Returns\n",
        "\n",
        "1. The exponential linear activation: x if x > 0 and alpha * (exp(x)-1) if x < 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFwOV5GJHUoB",
        "colab_type": "code",
        "outputId": "d1163c71-bb03-494d-fc42-8520fba05fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='elu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='elu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.2837 - acc: 0.9132 - val_loss: 0.1978 - val_acc: 0.9393\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1214 - acc: 0.9625 - val_loss: 0.1060 - val_acc: 0.9666\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0837 - acc: 0.9737 - val_loss: 0.0874 - val_acc: 0.9734\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0631 - acc: 0.9801 - val_loss: 0.0883 - val_acc: 0.9736\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0494 - acc: 0.9838 - val_loss: 0.0884 - val_acc: 0.9741\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0393 - acc: 0.9873 - val_loss: 0.1163 - val_acc: 0.9696\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0342 - acc: 0.9888 - val_loss: 0.0951 - val_acc: 0.9756\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0280 - acc: 0.9914 - val_loss: 0.0887 - val_acc: 0.9785\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0229 - acc: 0.9924 - val_loss: 0.0850 - val_acc: 0.9784\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0203 - acc: 0.9937 - val_loss: 0.0846 - val_acc: 0.9799\n",
            "Test loss: 0.08459654634964409\n",
            "Test accuracy: 0.9799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUN3hrzoKAmX",
        "colab_type": "text"
      },
      "source": [
        "#softmax\n",
        "\n",
        "keras.activations.softmax(x, axis=-1)\n",
        "\n",
        "##Softmax activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "2. axis: Integer, axis along which the softmax normalization is applied.\n",
        "\n",
        "##Returns\n",
        "\n",
        "Tensor, output of softmax transformation.\n",
        "\n",
        "##Raises\n",
        "\n",
        "ValueError: In case dim(x) == 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOQfshRuKK2k",
        "colab_type": "code",
        "outputId": "4176f06f-6abc-4f87-edc1-8eefbacc7689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='softmax', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='softmax'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 2.2688 - acc: 0.1762 - val_loss: 2.1954 - val_acc: 0.5574\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 2.0404 - acc: 0.7998 - val_loss: 1.8505 - val_acc: 0.8391\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 1.6371 - acc: 0.8393 - val_loss: 1.4163 - val_acc: 0.8450\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 1.2142 - acc: 0.8449 - val_loss: 1.0209 - val_acc: 0.8476\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.8645 - acc: 0.8485 - val_loss: 0.7276 - val_acc: 0.8503\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.6332 - acc: 0.8513 - val_loss: 0.5564 - val_acc: 0.8539\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.5087 - acc: 0.8550 - val_loss: 0.4721 - val_acc: 0.8572\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.4476 - acc: 0.8591 - val_loss: 0.4341 - val_acc: 0.8590\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.4162 - acc: 0.8635 - val_loss: 0.4114 - val_acc: 0.8630\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.3934 - acc: 0.8697 - val_loss: 0.3985 - val_acc: 0.8642\n",
            "Test loss: 0.39845509810447693\n",
            "Test accuracy: 0.8642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM19cjk8KTL7",
        "colab_type": "text"
      },
      "source": [
        "#selu\n",
        "\n",
        "keras.activations.selu(x)\n",
        "\n",
        "Scaled Exponential Linear Unit (SELU).\n",
        "\n",
        "SELU is equal to: scale * elu(x, alpha), where alpha and scale are predefined constants. The values of alpha and scale are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see lecun_normal initialization) and the number of inputs is \"large enough\" (see references for more information).\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: A tensor or variable to compute the activation function for.\n",
        "\n",
        "##Returns\n",
        "\n",
        "The scaled exponential unit activation: scale * elu(x, alpha).\n",
        "\n",
        "##Note\n",
        "\n",
        "1. To be used together with the initialization \"lecun_normal\".\n",
        "2. To be used together with the dropout variant \"AlphaDropout\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqueXLpaKddp",
        "colab_type": "code",
        "outputId": "936140ac-49ac-45fe-dab0-40b83c8d6305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='selu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='selu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3339 - acc: 0.8992 - val_loss: 0.2149 - val_acc: 0.9319\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.1353 - acc: 0.9590 - val_loss: 0.1086 - val_acc: 0.9673\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0938 - acc: 0.9714 - val_loss: 0.0887 - val_acc: 0.9730\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0720 - acc: 0.9772 - val_loss: 0.0953 - val_acc: 0.9693\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0592 - acc: 0.9810 - val_loss: 0.0808 - val_acc: 0.9751\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0486 - acc: 0.9841 - val_loss: 0.0808 - val_acc: 0.9755\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0415 - acc: 0.9861 - val_loss: 0.0862 - val_acc: 0.9766\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0361 - acc: 0.9882 - val_loss: 0.1075 - val_acc: 0.9702\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0296 - acc: 0.9904 - val_loss: 0.0941 - val_acc: 0.9779\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0265 - acc: 0.9913 - val_loss: 0.1185 - val_acc: 0.9726\n",
            "Test loss: 0.11848029121595427\n",
            "Test accuracy: 0.9726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jh93vQKKjws",
        "colab_type": "text"
      },
      "source": [
        "#softplus\n",
        "\n",
        "keras.activations.softplus(x)\n",
        "\n",
        "Softplus activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. The softplus activation: log(exp(x) + 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msVEMJUtKwTC",
        "colab_type": "code",
        "outputId": "c38e94ff-fbff-4e38-c5f1-f4a390cef734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='softplus', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='softplus'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.4494 - acc: 0.8619 - val_loss: 0.2046 - val_acc: 0.9345\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1736 - acc: 0.9470 - val_loss: 0.1176 - val_acc: 0.9628\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1119 - acc: 0.9659 - val_loss: 0.1073 - val_acc: 0.9688\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0833 - acc: 0.9737 - val_loss: 0.0896 - val_acc: 0.9730\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0651 - acc: 0.9795 - val_loss: 0.0861 - val_acc: 0.9739\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0521 - acc: 0.9838 - val_loss: 0.0838 - val_acc: 0.9745\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0435 - acc: 0.9862 - val_loss: 0.0778 - val_acc: 0.9793\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0356 - acc: 0.9884 - val_loss: 0.0732 - val_acc: 0.9812\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0305 - acc: 0.9904 - val_loss: 0.0779 - val_acc: 0.9806\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0254 - acc: 0.9919 - val_loss: 0.0801 - val_acc: 0.9798\n",
            "Test loss: 0.08013557475531798\n",
            "Test accuracy: 0.9798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5us_flAJK4lP",
        "colab_type": "text"
      },
      "source": [
        "#softsign\n",
        "\n",
        "keras.activations.softsign(x)\n",
        "\n",
        "##Softsign activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. The softsign activation: x / (abs(x) + 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxsE4bDzK_Wo",
        "colab_type": "code",
        "outputId": "2cba48b5-1794-4408-c8f3-7aea14b2dc5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='softsign', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='softsign'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.2944 - acc: 0.9135 - val_loss: 0.2009 - val_acc: 0.9404\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1445 - acc: 0.9561 - val_loss: 0.1240 - val_acc: 0.9613\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0995 - acc: 0.9694 - val_loss: 0.0943 - val_acc: 0.9720\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0743 - acc: 0.9774 - val_loss: 0.0856 - val_acc: 0.9748\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0576 - acc: 0.9829 - val_loss: 0.0816 - val_acc: 0.9745\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0451 - acc: 0.9861 - val_loss: 0.0749 - val_acc: 0.9755\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0357 - acc: 0.9888 - val_loss: 0.0757 - val_acc: 0.9781\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0281 - acc: 0.9913 - val_loss: 0.0677 - val_acc: 0.9786\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0228 - acc: 0.9933 - val_loss: 0.0666 - val_acc: 0.9802\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0174 - acc: 0.9950 - val_loss: 0.0716 - val_acc: 0.9793\n",
            "Test loss: 0.07156896027913608\n",
            "Test accuracy: 0.9793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1frWDGbLIPD",
        "colab_type": "text"
      },
      "source": [
        "#relu\n",
        "\n",
        "keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)\n",
        "\n",
        "##Rectified Linear Unit.\n",
        "\n",
        "With default values, it returns element-wise max(x, 0).\n",
        "\n",
        "Otherwise, it follows: f(x) = max_value for x >= max_value, f(x) = x for threshold <= x < max_value, f(x) = alpha * (x - threshold) otherwise.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "2. alpha: float. Slope of the negative part. Defaults to zero.\n",
        "3. max_value: float. Saturation threshold.\n",
        "4. threshold: float. Threshold value for thresholded activation.\n",
        "\n",
        "##Returns\n",
        "\n",
        "A tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-yB_H-6LEmI",
        "colab_type": "code",
        "outputId": "3ca4221f-68e7-4252-9584-922f3f28d797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.2273 - acc: 0.9310 - val_loss: 0.0998 - val_acc: 0.9688\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0850 - acc: 0.9739 - val_loss: 0.0734 - val_acc: 0.9770\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0540 - acc: 0.9834 - val_loss: 0.0756 - val_acc: 0.9774\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0396 - acc: 0.9877 - val_loss: 0.0855 - val_acc: 0.9770\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0294 - acc: 0.9909 - val_loss: 0.0770 - val_acc: 0.9783\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0217 - acc: 0.9931 - val_loss: 0.0800 - val_acc: 0.9818\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0173 - acc: 0.9945 - val_loss: 0.0734 - val_acc: 0.9832\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0158 - acc: 0.9951 - val_loss: 0.0861 - val_acc: 0.9823\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 0.1019 - val_acc: 0.9816\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0105 - acc: 0.9967 - val_loss: 0.0927 - val_acc: 0.9843\n",
            "Test loss: 0.09273777272929686\n",
            "Test accuracy: 0.9843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb6436cKLXyP",
        "colab_type": "text"
      },
      "source": [
        "#tanh\n",
        "\n",
        "keras.activations.tanh(x)\n",
        "\n",
        "##Hyperbolic tangent activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. The hyperbolic activation: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l_JVgdfLd_E",
        "colab_type": "code",
        "outputId": "c257faee-10e3-4a96-f2cc-f92d017a24c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='tanh', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='tanh'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2841 - acc: 0.9144 - val_loss: 0.1427 - val_acc: 0.9564\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.1235 - acc: 0.9628 - val_loss: 0.1045 - val_acc: 0.9681\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0815 - acc: 0.9750 - val_loss: 0.0951 - val_acc: 0.9704\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0592 - acc: 0.9819 - val_loss: 0.0830 - val_acc: 0.9730\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0434 - acc: 0.9870 - val_loss: 0.0757 - val_acc: 0.9766\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0317 - acc: 0.9904 - val_loss: 0.0732 - val_acc: 0.9768\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0235 - acc: 0.9930 - val_loss: 0.0725 - val_acc: 0.9773\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.0702 - val_acc: 0.9784\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0132 - acc: 0.9964 - val_loss: 0.0698 - val_acc: 0.9799\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0689 - val_acc: 0.9803\n",
            "Test loss: 0.06885148700817954\n",
            "Test accuracy: 0.9803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwPdvFC3LmME",
        "colab_type": "text"
      },
      "source": [
        "#sigmoid\n",
        "\n",
        "keras.activations.sigmoid(x)\n",
        "\n",
        "##Sigmoid activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. The sigmoid activation: 1 / (1 + exp(-x))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chkhKBK4LtS9",
        "colab_type": "code",
        "outputId": "b10716c2-da56-4ed7-ab09-71cdf668d434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='sigmoid', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.4923 - acc: 0.8595 - val_loss: 0.2385 - val_acc: 0.9275\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.2153 - acc: 0.9356 - val_loss: 0.1705 - val_acc: 0.9481\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1540 - acc: 0.9532 - val_loss: 0.1375 - val_acc: 0.9590\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1159 - acc: 0.9650 - val_loss: 0.1084 - val_acc: 0.9662\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0909 - acc: 0.9722 - val_loss: 0.0920 - val_acc: 0.9716\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0737 - acc: 0.9777 - val_loss: 0.0852 - val_acc: 0.9740\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0604 - acc: 0.9809 - val_loss: 0.0807 - val_acc: 0.9750\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0508 - acc: 0.9840 - val_loss: 0.0760 - val_acc: 0.9787\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0424 - acc: 0.9868 - val_loss: 0.0725 - val_acc: 0.9761\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0355 - acc: 0.9890 - val_loss: 0.0714 - val_acc: 0.9781\n",
            "Test loss: 0.07136517209208104\n",
            "Test accuracy: 0.9781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDSa6h2iL016",
        "colab_type": "text"
      },
      "source": [
        "#hard_sigmoid\n",
        "\n",
        "keras.activations.hard_sigmoid(x)\n",
        "\n",
        "##Hard sigmoid activation function.\n",
        "\n",
        "Faster to compute than sigmoid activation.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. Hard sigmoid activation:\n",
        "\n",
        "```\n",
        "0 if x < -2.5\n",
        "1 if x > 2.5\n",
        "0.2 * x + 0.5 if -2.5 <= x <= 2.5.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPIE0vA2MIY0",
        "colab_type": "code",
        "outputId": "d0ae3a0e-7895-4905-a264-58d28c85dbfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='hard_sigmoid', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='hard_sigmoid'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.5035 - acc: 0.8561 - val_loss: 0.2442 - val_acc: 0.9312\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.2098 - acc: 0.9377 - val_loss: 0.1812 - val_acc: 0.9448\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.1464 - acc: 0.9555 - val_loss: 0.1251 - val_acc: 0.9609\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.1090 - acc: 0.9679 - val_loss: 0.1087 - val_acc: 0.9685\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0854 - acc: 0.9743 - val_loss: 0.0956 - val_acc: 0.9717\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0685 - acc: 0.9795 - val_loss: 0.0842 - val_acc: 0.9753\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0554 - acc: 0.9833 - val_loss: 0.0891 - val_acc: 0.9741\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0451 - acc: 0.9865 - val_loss: 0.0742 - val_acc: 0.9777\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0370 - acc: 0.9889 - val_loss: 0.0727 - val_acc: 0.9785\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0304 - acc: 0.9906 - val_loss: 0.0697 - val_acc: 0.9795\n",
            "Test loss: 0.06968761095895898\n",
            "Test accuracy: 0.9795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkEibscrMTJ7",
        "colab_type": "text"
      },
      "source": [
        "#exponential\n",
        "\n",
        "keras.activations.exponential(x)\n",
        "\n",
        "Exponential (base e) activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. Exponential activation: exp(x)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z03_Vua1MY82",
        "colab_type": "code",
        "outputId": "ec2bf4c6-af04-4252-fdf9-e17eac726095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='exponential', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='exponential'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 14.5152 - acc: 0.0984 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "Test loss: 14.573981648254394\n",
            "Test accuracy: 0.0958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twHrIcibMfqj",
        "colab_type": "text"
      },
      "source": [
        "#linear\n",
        "\n",
        "keras.activations.linear(x)\n",
        "\n",
        "Linear (i.e. identity) activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "Input tensor, unchanged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4yIoO3RMk-n",
        "colab_type": "code",
        "outputId": "c1a60092-c74d-4d00-cf59-15336a82390b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='linear', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='linear'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.4296 - acc: 0.8761 - val_loss: 0.3429 - val_acc: 0.9017\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.3427 - acc: 0.9025 - val_loss: 0.3263 - val_acc: 0.9066\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.3219 - acc: 0.9094 - val_loss: 0.3062 - val_acc: 0.9147\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.3074 - acc: 0.9131 - val_loss: 0.3242 - val_acc: 0.9099\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.3002 - acc: 0.9156 - val_loss: 0.2907 - val_acc: 0.9205\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.2951 - acc: 0.9168 - val_loss: 0.2933 - val_acc: 0.9198\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.2892 - acc: 0.9189 - val_loss: 0.2924 - val_acc: 0.9169\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.2854 - acc: 0.9205 - val_loss: 0.3159 - val_acc: 0.9138\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.2821 - acc: 0.9222 - val_loss: 0.2929 - val_acc: 0.9203\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.2777 - acc: 0.9236 - val_loss: 0.3017 - val_acc: 0.9158\n",
            "Test loss: 0.30165807257443666\n",
            "Test accuracy: 0.9158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OyPjtogMtdG",
        "colab_type": "text"
      },
      "source": [
        "#LeakyReLU\n",
        "\n",
        "keras.layers.LeakyReLU(alpha=0.3)\n",
        "\n",
        "Leaky version of a Rectified Linear Unit.\n",
        "\n",
        "It allows a small gradient when the unit is not active: \n",
        "```\n",
        "f(x) = alpha * x for x < 0\n",
        "f(x) = x for x >= 0.\n",
        "```\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "alpha: float >= 0. Negative slope coefficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erjq0UrxNRum",
        "colab_type": "code",
        "outputId": "0a2d10f1-7292-493a-f395-b69449ee8c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.LeakyReLU(alpha=0.3), input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.LeakyReLU(alpha=0.3)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.2676 - acc: 0.9182 - val_loss: 0.1356 - val_acc: 0.9603\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1272 - acc: 0.9612 - val_loss: 0.1250 - val_acc: 0.9605\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0929 - acc: 0.9712 - val_loss: 0.0994 - val_acc: 0.9700\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0740 - acc: 0.9773 - val_loss: 0.0925 - val_acc: 0.9728\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0638 - acc: 0.9800 - val_loss: 0.1135 - val_acc: 0.9647\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0547 - acc: 0.9830 - val_loss: 0.0860 - val_acc: 0.9750\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0481 - acc: 0.9849 - val_loss: 0.0994 - val_acc: 0.9737\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0427 - acc: 0.9863 - val_loss: 0.0970 - val_acc: 0.9732\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0368 - acc: 0.9877 - val_loss: 0.0892 - val_acc: 0.9769\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0341 - acc: 0.9891 - val_loss: 0.1155 - val_acc: 0.9709\n",
            "Test loss: 0.11553157684192265\n",
            "Test accuracy: 0.9709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJBHILnNv8k",
        "colab_type": "text"
      },
      "source": [
        "#PReLU\n",
        "\n",
        "keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
        "\n",
        "Parametric Rectified Linear Unit.\n",
        "\n",
        "It follows: f(x) = alpha * x for x < 0, f(x) = x for x >= 0, where alpha is a learned array with the same shape as x.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. alpha_initializer: initializer function for the weights.\n",
        "2. alpha_regularizer: regularizer for the weights.\n",
        "3. alpha_constraint: constraint for the weights.\n",
        "4. shared_axes: the axes along which to share learnable parameters for the activation function. For example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels), and you wish to share parameters across space so that each filter only has one set of parameters, set shared_axes=[1, 2]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sMj5qFNN7Xv",
        "colab_type": "code",
        "outputId": "bdf41eac-2224-4b76-f028-de767f453a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
        ", input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
        "))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as PReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 42us/step - loss: 0.2256 - acc: 0.9319 - val_loss: 0.1013 - val_acc: 0.9679\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0843 - acc: 0.9738 - val_loss: 0.0766 - val_acc: 0.9780\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0557 - acc: 0.9828 - val_loss: 0.0683 - val_acc: 0.9809\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0393 - acc: 0.9876 - val_loss: 0.0848 - val_acc: 0.9777\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0291 - acc: 0.9908 - val_loss: 0.0822 - val_acc: 0.9769\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0223 - acc: 0.9932 - val_loss: 0.0828 - val_acc: 0.9809\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0180 - acc: 0.9943 - val_loss: 0.0861 - val_acc: 0.9807\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0964 - val_acc: 0.9802\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0119 - acc: 0.9964 - val_loss: 0.0886 - val_acc: 0.9826\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.1010 - val_acc: 0.9824\n",
            "Test loss: 0.10103868100494455\n",
            "Test accuracy: 0.9824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZFbSZYOSCi",
        "colab_type": "text"
      },
      "source": [
        "#ELU\n",
        "\n",
        "keras.layers.ELU(alpha=1.0)\n",
        "\n",
        "Exponential Linear Unit.\n",
        "\n",
        "It follows: f(x) =  alpha * (exp(x) - 1.) for x < 0, f(x) = x for x >= 0.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. alpha: scale for the negative factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV_7KHrmOZEV",
        "colab_type": "code",
        "outputId": "9b66dfc1-0d92-4300-c3a6-102e25bdde90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.ELU(alpha=1.0), input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.ELU(alpha=1.0)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as ELU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.2918 - acc: 0.9116 - val_loss: 0.1373 - val_acc: 0.9585\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1221 - acc: 0.9625 - val_loss: 0.1112 - val_acc: 0.9645\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0832 - acc: 0.9746 - val_loss: 0.0935 - val_acc: 0.9699\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0632 - acc: 0.9800 - val_loss: 0.0810 - val_acc: 0.9754\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0505 - acc: 0.9840 - val_loss: 0.0869 - val_acc: 0.9725\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0409 - acc: 0.9867 - val_loss: 0.0956 - val_acc: 0.9735\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0335 - acc: 0.9892 - val_loss: 0.0757 - val_acc: 0.9796\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0275 - acc: 0.9909 - val_loss: 0.0984 - val_acc: 0.9772\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0226 - acc: 0.9928 - val_loss: 0.0873 - val_acc: 0.9779\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0196 - acc: 0.9937 - val_loss: 0.1115 - val_acc: 0.9743\n",
            "Test loss: 0.1115254690002792\n",
            "Test accuracy: 0.9743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMMit6nZOlXi",
        "colab_type": "text"
      },
      "source": [
        "#ThresholdedReLU\n",
        "\n",
        "keras.layers.ThresholdedReLU(theta=1.0)\n",
        "\n",
        "Thresholded Rectified Linear Unit.\n",
        "\n",
        "It follows: f(x) = x for x > theta, f(x) = 0 otherwise.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. theta: float >= 0. Threshold location of activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtN3bUOOOqN0",
        "colab_type": "code",
        "outputId": "2a4595ac-efb4-4640-eec8-03d062346a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.ThresholdedReLU(theta=1.0), input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.ThresholdedReLU(theta=1.0)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as ThresholdedReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.5300 - acc: 0.8282 - val_loss: 0.1811 - val_acc: 0.9435\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.1445 - acc: 0.9572 - val_loss: 0.1260 - val_acc: 0.9606\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0979 - acc: 0.9705 - val_loss: 0.1333 - val_acc: 0.9594\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0754 - acc: 0.9769 - val_loss: 0.1080 - val_acc: 0.9686\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0598 - acc: 0.9818 - val_loss: 0.0861 - val_acc: 0.9749\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0490 - acc: 0.9850 - val_loss: 0.0814 - val_acc: 0.9783\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0410 - acc: 0.9875 - val_loss: 0.0754 - val_acc: 0.9773\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0337 - acc: 0.9897 - val_loss: 0.0795 - val_acc: 0.9802\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0286 - acc: 0.9910 - val_loss: 0.0831 - val_acc: 0.9797\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0239 - acc: 0.9924 - val_loss: 0.0848 - val_acc: 0.9786\n",
            "Test loss: 0.08483904730657359\n",
            "Test accuracy: 0.9786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHf3ajyEOy7R",
        "colab_type": "text"
      },
      "source": [
        "#Softmax\n",
        "\n",
        "keras.layers.Softmax(axis=-1)\n",
        "\n",
        "Softmax activation function.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. axis: Integer, axis along which the softmax normalization is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSzMbFfGO600",
        "colab_type": "code",
        "outputId": "41ff4b8c-94f6-446a-d3c5-69b80bc3d8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.Softmax(axis=-1), input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.Softmax(axis=-1)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as Softmax) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.2692 - acc: 0.1798 - val_loss: 2.1970 - val_acc: 0.6770\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 2.0416 - acc: 0.8212 - val_loss: 1.8493 - val_acc: 0.8815\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 1.6269 - acc: 0.9013 - val_loss: 1.3929 - val_acc: 0.9135\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 1.1774 - acc: 0.9164 - val_loss: 0.9659 - val_acc: 0.9210\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.7995 - acc: 0.9228 - val_loss: 0.6472 - val_acc: 0.9248\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.5441 - acc: 0.9257 - val_loss: 0.4570 - val_acc: 0.9273\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.4040 - acc: 0.9286 - val_loss: 0.3627 - val_acc: 0.9304\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.3365 - acc: 0.9311 - val_loss: 0.3223 - val_acc: 0.9308\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.3024 - acc: 0.9331 - val_loss: 0.2959 - val_acc: 0.9314\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.2806 - acc: 0.9362 - val_loss: 0.2819 - val_acc: 0.9333\n",
            "Test loss: 0.2818538703083992\n",
            "Test accuracy: 0.9333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCbXH5M2PCwO",
        "colab_type": "text"
      },
      "source": [
        "#ReLU\n",
        "\n",
        "keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        "\n",
        "Rectified Linear Unit activation function.\n",
        "\n",
        "With default values, it returns element-wise max(x, 0).\n",
        "\n",
        "Otherwise, it follows: f(x) = max_value for x >= max_value, f(x) = x for threshold <= x < max_value, f(x) = negative_slope * (x - threshold) otherwise.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. max_value: float >= 0. Maximum activation value.\n",
        "2. negative_slope: float >= 0. Negative slope coefficient.\n",
        "3. threshold: float. Threshold value for thresholded activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMee8FeqPK2N",
        "colab_type": "code",
        "outputId": "d958a2a0-4e73-464c-dea8-7f62fae564aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        ", input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        "))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.2252 - acc: 0.9315 - val_loss: 0.1103 - val_acc: 0.9672\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0822 - acc: 0.9747 - val_loss: 0.0826 - val_acc: 0.9739\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0545 - acc: 0.9827 - val_loss: 0.0948 - val_acc: 0.9718\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0389 - acc: 0.9879 - val_loss: 0.0732 - val_acc: 0.9782\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0290 - acc: 0.9911 - val_loss: 0.0891 - val_acc: 0.9766\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0220 - acc: 0.9931 - val_loss: 0.0734 - val_acc: 0.9819\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0179 - acc: 0.9945 - val_loss: 0.0821 - val_acc: 0.9819\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.0841 - val_acc: 0.9815\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0120 - acc: 0.9966 - val_loss: 0.1162 - val_acc: 0.9789\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0921 - val_acc: 0.9829\n",
            "Test loss: 0.09205757192509677\n",
            "Test accuracy: 0.9829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw_XEgUlPU04",
        "colab_type": "text"
      },
      "source": [
        "#Thank you for completing this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb9dX1z1QIWA",
        "colab_type": "text"
      },
      "source": [
        "**relu** \n",
        "\n",
        "Test loss: 0.08459654634964409\n",
        "\n",
        "Test accuracy: 0.9799\n",
        "\n",
        "**softmax**\n",
        "\n",
        "Test loss: 0.39845509810447693\n",
        "\n",
        "Test accuracy: 0.8642\n",
        "\n",
        "**selu**\n",
        "\n",
        "Test loss: 0.11848029121595427\n",
        "\n",
        "Test accuracy: 0.9726\n",
        "\n",
        "**softplus**\n",
        "\n",
        "Test loss: 0.08013557475531798\n",
        "\n",
        "Test accuracy: 0.9798\n",
        "\n",
        "**softsign**\n",
        "\n",
        "Test loss: 0.07156896027913608\n",
        "\n",
        "Test accuracy: 0.9793\n",
        "\n",
        "**relu**\n",
        "\n",
        "Test loss: 0.09273777272929686\n",
        "\n",
        "Test accuracy: 0.9843\n",
        "\n",
        "**tanh**\n",
        "\n",
        "Test loss: 0.06885148700817954\n",
        "\n",
        "Test accuracy: 0.9803\n",
        "\n",
        "**sigmoid**\n",
        "\n",
        "Test loss: 0.07136517209208104\n",
        "\n",
        "Test accuracy: 0.9781\n",
        "\n",
        "**hard_sigmoid**\n",
        "\n",
        "Test loss: 0.06968761095895898\n",
        "\n",
        "Test accuracy: 0.9795\n",
        "\n",
        "**exponential**\n",
        "\n",
        "Test loss: 14.573981648254394\n",
        "\n",
        "Test accuracy: 0.0958\n",
        "\n",
        "**linear**\n",
        "\n",
        "Test loss: 0.30165807257443666\n",
        "\n",
        "Test accuracy: 0.9158\n",
        "\n",
        "**LeakyReLU**\n",
        "\n",
        "Test loss: 0.11553157684192265\n",
        "\n",
        "Test accuracy: 0.9709\n",
        "\n",
        "**PReLU**\n",
        "\n",
        "Test loss: 0.11553157684192265\n",
        "\n",
        "Test accuracy: 0.9709\n",
        "\n",
        "**ELU**\n",
        "\n",
        "Test loss: 0.1115254690002792\n",
        "\n",
        "Test accuracy: 0.9743\n",
        "\n",
        "**ThresholdedReLU**\n",
        "\n",
        "Test loss: 0.08483904730657359\n",
        "\n",
        "Test accuracy: 0.9786\n",
        "\n",
        "**Softmax**\n",
        "\n",
        "Test loss: 0.2818538703083992\n",
        "\n",
        "Test accuracy: 0.9333\n",
        "\n",
        "**ReLU**\n",
        "\n",
        "Test loss: 0.09205757192509677\n",
        "\n",
        "Test accuracy: 0.9829\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gsUybYNSlEt",
        "colab_type": "text"
      },
      "source": [
        "The best one is **ReLU** among all the activations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhCdPdvQTGAt",
        "colab_type": "code",
        "outputId": "45c66287-0588-48a7-e363-1ef322f5a864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "import keras\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "from keras.layers import Activation, Flatten, Conv2D\n",
        "\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(512, (28, 28), padding=\"valid\", activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0), input_shape=X_train.shape[1:]))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        ", input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        "))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 138us/step - loss: 0.5952 - acc: 0.7774 - val_loss: 0.4582 - val_acc: 0.8405\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.3959 - acc: 0.8543 - val_loss: 0.4307 - val_acc: 0.8490\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.3511 - acc: 0.8709 - val_loss: 0.3689 - val_acc: 0.8703\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.3268 - acc: 0.8806 - val_loss: 0.3826 - val_acc: 0.8659\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.3104 - acc: 0.8866 - val_loss: 0.4175 - val_acc: 0.8533\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2976 - acc: 0.8906 - val_loss: 0.3873 - val_acc: 0.8679\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2883 - acc: 0.8938 - val_loss: 0.4067 - val_acc: 0.8640\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2801 - acc: 0.8982 - val_loss: 0.4000 - val_acc: 0.8716\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2737 - acc: 0.8989 - val_loss: 0.4326 - val_acc: 0.8617\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2676 - acc: 0.9034 - val_loss: 0.4138 - val_acc: 0.8699\n",
            "Test loss: 0.4138208844304085\n",
            "Test accuracy: 0.8699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbXrhH1UIrR",
        "colab_type": "text"
      },
      "source": [
        "**FASHION_MNIST**\n",
        "\n",
        "Test loss: 0.4138208844304085\n",
        "\n",
        "Test accuracy: 0.8699"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcBaoBWNTRKI",
        "colab_type": "code",
        "outputId": "1e950cc4-eb8f-46c5-8799-dfc09e523ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "import keras\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout, Conv2D,Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "#import dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#change shape from image to vector\n",
        "X_train = X_train.reshape(50000,32,32,3)\n",
        "X_test = X_test.reshape(10000,32,32,3)\n",
        "\n",
        "#preprocess\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "#change labels from numeric to one hot encoded\n",
        "Y_train = to_categorical(y_train, 10)\n",
        "Y_test =  to_categorical(y_test, 10)\n",
        "\n",
        "adamax=keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "\n",
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(1024, (32, 32), padding=\"valid\", activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0), input_shape=X_train.shape[1:]))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        ", input_shape=(32 * 32,)))\n",
        "model.add(Dense(256, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        "))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adamax',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 2.0376 - acc: 0.2890 - val_loss: 1.7873 - val_acc: 0.3573\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 4s 70us/step - loss: 1.7179 - acc: 0.3877 - val_loss: 1.6877 - val_acc: 0.4077\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 4s 71us/step - loss: 1.6274 - acc: 0.4201 - val_loss: 1.6767 - val_acc: 0.4093\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 3s 70us/step - loss: 1.5713 - acc: 0.4439 - val_loss: 1.5704 - val_acc: 0.4346\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 3s 70us/step - loss: 1.5193 - acc: 0.4598 - val_loss: 1.5190 - val_acc: 0.4570\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 3s 69us/step - loss: 1.4744 - acc: 0.4746 - val_loss: 1.4632 - val_acc: 0.4809\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 3s 68us/step - loss: 1.4309 - acc: 0.4923 - val_loss: 1.4382 - val_acc: 0.4868\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 3s 68us/step - loss: 1.3917 - acc: 0.5049 - val_loss: 1.4318 - val_acc: 0.4862\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 3s 69us/step - loss: 1.3517 - acc: 0.5202 - val_loss: 1.4254 - val_acc: 0.4982\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 3s 69us/step - loss: 1.3223 - acc: 0.5282 - val_loss: 1.4010 - val_acc: 0.4971\n",
            "Test loss: 1.4009930366516112\n",
            "Test accuracy: 0.4971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQL5ODwGbh2X",
        "colab_type": "text"
      },
      "source": [
        "**CIFAR10**\n",
        "\n",
        "Test loss: 1.4009930366516112\n",
        "\n",
        "Test accuracy: 0.4971\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDb0FMDmV6Gp",
        "colab_type": "code",
        "outputId": "1c43d8f4-7ddc-4a47-b146-4dbd536a6a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "import keras\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout, Conv2D,Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import cifar100\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "#import dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "\n",
        "#change shape from image to vector\n",
        "X_train = X_train.reshape(50000,32,32,3)\n",
        "X_test = X_test.reshape(10000,32,32,3)\n",
        "\n",
        "#preprocess\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "#change labels from numeric to one hot encoded\n",
        "Y_train = to_categorical(y_train, 100)\n",
        "Y_test =  to_categorical(y_test, 100)\n",
        "\n",
        "adamax=keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "\n",
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(1024, (32, 32), padding=\"valid\", activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0), input_shape=X_train.shape[1:]))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        ", input_shape=(32 * 32,)))\n",
        "model.add(Dense(256, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        "))\n",
        "model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adamax',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 4.3099 - acc: 0.0508 - val_loss: 4.0004 - val_acc: 0.0884\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 4s 70us/step - loss: 3.8491 - acc: 0.1114 - val_loss: 3.7959 - val_acc: 0.1133\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 4s 72us/step - loss: 3.6442 - acc: 0.1473 - val_loss: 3.5849 - val_acc: 0.1562\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 4s 72us/step - loss: 3.5108 - acc: 0.1681 - val_loss: 3.5200 - val_acc: 0.1725\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 4s 73us/step - loss: 3.3917 - acc: 0.1911 - val_loss: 3.4822 - val_acc: 0.1804\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 4s 71us/step - loss: 3.2977 - acc: 0.2070 - val_loss: 3.3675 - val_acc: 0.2014\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 4s 72us/step - loss: 3.2009 - acc: 0.2235 - val_loss: 3.2949 - val_acc: 0.2189\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 4s 72us/step - loss: 3.1242 - acc: 0.2363 - val_loss: 3.2911 - val_acc: 0.2220\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 4s 74us/step - loss: 3.0533 - acc: 0.2502 - val_loss: 3.2471 - val_acc: 0.2260\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 4s 73us/step - loss: 2.9818 - acc: 0.2656 - val_loss: 3.2423 - val_acc: 0.2326\n",
            "Test loss: 3.242277897644043\n",
            "Test accuracy: 0.2326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcgYinKudM9Y",
        "colab_type": "text"
      },
      "source": [
        "**CIFAR100**\n",
        "\n",
        "Test loss: 3.242277897644043\n",
        "\n",
        "Test accuracy: 0.2326"
      ]
    }
  ]
}